---
import Layout from "../../../components/Layout.astro";
const BASE = import.meta.env.BASE_URL;

// Edit these strings/paths later
const project = {
  title: "Prosody-Aware Robot Intervention",
  subtitle: "Detecting vocal hesitancy in real time to improve human–robot collaboration",
  date: "June 2025–Present",
  category: "Human–Robot Interaction, Research, Controls, User Studies",
  tools: [
    "Python",
    "Flask",
    "Hugging Face",
    "ROS 2"
  ],
  links: [
    { label: "Contraction Control in Prosody-Driven HRI (formal paper)", href: `${BASE}projects/prosody-hri/2_165_Final_Paper.pdf` },
  ],
  gallery: [
    // Replace with your actual assets
    { src: `${BASE}projects/prosody-hri/hero-1.png`, mode: "cover" },   // e.g., UI screenshot or pipeline graphic
    { src: `${BASE}projects/prosody-hri/hero-2.png`, mode: "contain" }, // e.g., task grid/stations diagram
  ],
};
---

<Layout title={`${project.title} — River Adkins`}>
  <section class="panel project-page">
    <!-- ===================== -->
    <!-- Back -->
    <!-- ===================== -->
    <a class="back" href={`${import.meta.env.BASE_URL}#projects`}>← Back to projects</a>

    <!-- ===================== -->
    <!-- Header -->
    <!-- ===================== -->
    <header class="project-header">
      <h1 class="project-title">{project.title}</h1>
      <p class="project-subtitle">{project.subtitle}</p>
    </header>

    <!-- ===================== -->
    <!-- Top: Gallery + Info -->
    <!-- ===================== -->
    <div class="project-top">
      <!-- Gallery -->
      <div class="gallery" data-gallery>
        <div class="gallery-track">
          {project.gallery.map((item) => (
            <div class={`gallery-slide ${item.mode === "contain" ? "is-poster" : ""}`}>
              <img src={item.src} alt="" loading="lazy" />
            </div>
          ))}
        </div>

        <div class="gallery-controls">
          <button type="button" class="gallery-btn" data-prev aria-label="Previous">←</button>
          <button type="button" class="gallery-btn" data-next aria-label="Next">→</button>
        </div>

        <div class="gallery-dots" data-dots></div>
      </div>

      <!-- Project info card -->
      <aside class="project-card">
        <h2 class="project-card-title">Project Info</h2>
        <hr />
        <p><strong>Category:</strong> {project.category}</p>
        <p><strong>Project date:</strong> {project.date}</p>
        <p><strong>Tools used:</strong> {project.tools.join(", ")}</p>

        <div class="project-links">
          {project.links.map((l) => (
            <a
              class="link"
              href={l.href}
              target={l.href.startsWith("http") ? "_blank" : undefined}
              rel="noreferrer"
            >
              {l.label} →
            </a>
          ))}
        </div>
      </aside>
    </div>

    <!-- ===================== -->
    <!-- Body -->
    <!-- ===================== -->
    <article class="project-body">
    <section class="project-background">
    <h2>Background</h2>

    <p>
        After building an autonomous mobile robot platform in Spring 2025, I began focusing more on the
        human side. How people give instructions, where uncertainty shows up in speech, and
        why speech-driven robot systems often behave unpredictably.
    </p>

    <p>
        Reading the HRI and speech prosody literature revealed a gap in implementation on real robots and real user testing. I’ve been working to address
        this gap through system building, simulation, and theory.
    </p>

    <p>
        This past fall, I also took a graduate-level robotics course (2.165) which taught me some nonlinear control methods. I used the final
        paper as an opportunity to connect that material directly to my research, resulting in a
        contractive controller that supports prosody-aware robot interaction.
    </p>
    </section>

      <!-- Overview -->
      <section>
        <h2>Project Overview</h2>

        <p>
          Human–robot interaction isn’t only about what people say, but how they say it.
          This research investigates whether real-time detection of vocal hesitancy can help a robot identify when a user
          is likely to make an error, and intervene with lightweight clarifications at the right moment.
        </p>

        <p>
          The core goal is to build an interaction loop where the robot becomes a better collaborator. It notices uncertainty,
          asks for confirmation when it matters, and improves both task accuracy and user experience.
        </p>
      </section>

      <!-- Research Objective + Hypotheses -->
      <section>
        <h2>Research Objective</h2>

        <p>
          This study evaluates whether hesitancy in speech can serve as a reliable signal of uncertainty during
          sequential recall tasks, and whether a robot can use that signal to intervene with clarifying questions
          (like “Are you sure?”) to reduce errors and increase satisfaction.
        </p>

        <p>Primary hypotheses:</p>
        <ul class="arch-list">
          <li><strong>H1:</strong> Vocal hesitancy increases when participants begin to forget steps in a sequence.</li>
          <li><strong>H2:</strong> Hesitancy-triggered robot interventions reduce error rates.</li>
          <li><strong>H3:</strong> Participants prefer hesitancy-based interventions over random or no intervention.</li>
        </ul>
      </section>

      <!-- Study Design -->
      <section>
        <h2>User Study Direction</h2>

        <p>
            This project is evolving toward a controlled user study that evaluates whether real-time detection
            of vocal hesitancy can help a robot intervene at moments of uncertainty and improve collaboration.
            Rather than treating speech errors after they occur, the goal is to detect uncertainty in real-time
            and support the user before mistakes are made.
        </p>

        <p>
            Participants interact with a robot in both a simulated and a physical environment while completing
            structured, multi-step tasks that place increasing demands on memory. As cognitive
            load rises, natural hesitancy emerges in speech, providing a signal for potential intervention.
        </p>

        <p>
            The robot compares three interaction strategies: no intervention, random clarification, and
            hesitancy-triggered clarification. Performance and user preference will be evaluated to determine
            whether prosody-aware intervention improves accuracy without becoming disruptive. The study is anticipated to occur this upcoming March.
        </p>
        </section>

      <!-- Hesitancy Detection Pipeline -->
      <section id="hesitancy-pipeline">
        <h2>Hesitancy Detection Pipeline</h2>

        <p>
          Speech is captured in real time and transformed into a continuous hesitancy probability signal. A tunable threshold
          triggers robot clarification when uncertainty is likely.
        </p>

          <div>
            <h3>Pipeline stages</h3>
            <ul class="arch-list">
            <li><strong>Audio capture:</strong> live microphone input during recall.</li>
            <li><strong>Parallel inference:</strong> the raw audio stream is sent (1) directly into the speech-prosody classifier and (2) through Whisper for an exact transcript.</li>
            <li><strong>Multi-signal outputs:</strong> the classifier returns multiple probability scores characterizing the speech (soft, authoritative, loud, etc), rather than a single “hesitancy” label.</li>
            <li><strong>Uncertainty aggregation:</strong> a custom algorithm combines these outputs into a single hesitancy probability for the command.</li>
            <li><strong>Intervention logic:</strong> if the uncertainty score exceeds a certain threshold, the robot asks for confirmation.</li>
            </ul>

          </div>

      </section>

      <!-- What I Built / Role -->
      <section>
        <div>
          <h2>My Role</h2>
          <p>
            I’m building the end-to-end interaction system: the simulation environment (using Flask) for controlled trials, the real-time
            audio + prosody pipeline, and the robot-side intervention logic that closes the loop during sequence recall. I am implementing all this on the mobile platform that I developed last spring.
          </p>
          <p>
            In parallel, I applied nonlinear control concepts from a graduate-level robotics course that I took this fall to build a stable motion baseline, so that variability from human interaction could be studied without being confounded by control instability.
            For my final paper for the class, I wrote about this design which you can read <a
              class="link"
              href={`${BASE}projects/prosody-hri/2_165_Final_Paper.pdf`}
              target="_blank"
              rel="noreferrer"
            >
              here.
            </a>
            
          </p>
        </div>

        <div class="diagram-card">
          <img
            src={`${BASE}projects/prosody-hri/controller.png`}
            alt="Graph comparing the contraction controller to traditional methods."
            loading="lazy"
          />
          <p class="diagram-caption">
          Tracking of hesitancy-induced oscillations in the lateral direction. The contraction-based
            controller (dashed line) follows the perturbed reference closely, while the baseline controller (dotted
            line) lags and fails to capture the oscillatory structure produced by prosodic hesitancy.
          </p>
        </div>
      </section>

      <!-- Future Work -->
      <section class="proj-section">
        <h2 class="proj-h2">Future Work</h2>
        <p>
          Next steps focus on finalizing the integrated system and conducting the user study.
        </p>
        <p>
          Longer-term, this work extends toward prosody-aware autonomy: integrating richer prosodic states (confidence, urgency, sarcasm)
          and designing robot behaviors that adapt in real time (clarify, slow down, re-plan, or ask follow-up questions) based on how
          instructions are delivered.
        </p>
      </section>
    </article>
  </section>
</Layout>

<script is:inline>
(() => {
  const root = document.querySelector("[data-gallery]");
  if (!root) return;

  const track = root.querySelector(".gallery-track");
  const slides = [...root.querySelectorAll(".gallery-slide")];
  const prev = root.querySelector("[data-prev]");
  const next = root.querySelector("[data-next]");
  const dotsWrap = root.querySelector("[data-dots]");

  if (!track || slides.length === 0) return;

  let idx = 0;

  function setActiveDot() {
    if (!dotsWrap) return;
    [...dotsWrap.children].forEach((d, j) => d.toggleAttribute("data-active", j === idx));
  }

  function snapTo(i) {
    idx = (i + slides.length) % slides.length;

    slides[idx].scrollIntoView({
      behavior: "smooth",
      inline: "start",
      block: "nearest",
    });

    setActiveDot();
  }

  if (dotsWrap) {
    dotsWrap.innerHTML = "";
    slides.forEach((_, i) => {
      const b = document.createElement("button");
      b.type = "button";
      b.className = "gallery-dot";
      b.addEventListener("click", (e) => {
        e.preventDefault();
        snapTo(i);
      });
      dotsWrap.appendChild(b);
    });
  }

  prev?.addEventListener("click", (e) => {
    e.preventDefault();
    snapTo(idx - 1);
  });

  next?.addEventListener("click", (e) => {
    e.preventDefault();
    snapTo(idx + 1);
  });

  root.addEventListener("keydown", (e) => {
    if (e.key === "ArrowLeft") {
      e.preventDefault();
      snapTo(idx - 1);
    } else if (e.key === "ArrowRight") {
      e.preventDefault();
      snapTo(idx + 1);
    }
  });

  if (!root.hasAttribute("tabindex")) root.setAttribute("tabindex", "0");

  snapTo(0);

  const videos = document.querySelectorAll("video.lazy-video[data-src]");
  if (!videos.length) return;

  const loadVideo = (video) => {
    const src = video.dataset.src;
    if (!src) return;
    video.src = src;
    video.removeAttribute("data-src");
  };

  if (!("IntersectionObserver" in window)) {
    videos.forEach(loadVideo);
    return;
  }

  const io = new IntersectionObserver((entries) => {
    entries.forEach((entry) => {
      if (!entry.isIntersecting) return;
      loadVideo(entry.target);
      io.unobserve(entry.target);
    });
  }, { rootMargin: "200px 0px" });

  videos.forEach(v => io.observe(v));
})();
</script>
