---
import Layout from "../../../components/Layout.astro";
const BASE = import.meta.env.BASE_URL;

// Edit these strings later
const project = {
  title: "HRI User Studies with Social Robots",
  subtitle: "MIT Personal Robotics Group at the Media Lab",
  date: "June-Oct 2023",
  category: "Research, Human-Robot Interaction",
  tools: ["User studies", "Data analysis"],
  links: [
    { label: "Read HRI 2025 Paper Here", href: "https://arxiv.org/abs/2502.00221" },
  ],

gallery: [
  { src: `${BASE}projects/media-lab/hero-1.jpg`, mode: "contain" },
  { src: `${BASE}projects/media-lab/hero-2.png`, mode: "cover" },
    { src: `${BASE}projects/media-lab/JiboCat.gif`, mode: "contain" },

],

};
---

<Layout title={`${project.title} — River Adkins`}>
  <section class="panel project-page">
    <!-- ===================== -->
    <!-- Back -->
    <!-- ===================== -->
    <a class="back" href={`${import.meta.env.BASE_URL}#projects`}>← Back to projects</a>

    <!-- ===================== -->
    <!-- Header -->
    <!-- ===================== -->
    <header class="project-header">
      <h1 class="project-title">{project.title}</h1>
      <p class="project-subtitle">{project.subtitle}</p>
    </header>

    <!-- ===================== -->
    <!-- Top: Gallery + Info -->
    <!-- ===================== -->
    <div class="project-top">
      <!-- Gallery -->
      <div class="gallery" data-gallery>
        <div class="gallery-track">
          {project.gallery.map((item) => (
            <div class={`gallery-slide ${item.mode === "contain" ? "is-poster" : ""}`}>
                <img src={item.src} alt="" loading="lazy" />
            </div>
            ))}

        </div>

        <div class="gallery-controls">
        <button type="button" class="gallery-btn" data-prev aria-label="Previous">←</button>
        <button type="button" class="gallery-btn" data-next aria-label="Next">→</button>
        </div>


        <div class="gallery-dots" data-dots></div>
      </div>

      <!-- Project info card -->
      <aside class="project-card">
        <h2 class="project-card-title">Social Robots</h2>
        <hr />
        <p><strong>Category:</strong> {project.category}</p>
        <p><strong>Project date:</strong> {project.date}</p>
        <p><strong>Tools used:</strong> {project.tools.join(", ")}</p>

        <div class="project-links">
          {project.links.map((l) => (
            <a
              class="link"
              href={l.href}
              target={l.href.startsWith("http") ? "_blank" : undefined}
              rel="noreferrer"
            >
              {l.label} →
            </a>
          ))}
        </div>
      </aside>
    </div>

    <!-- ===================== -->
    <!-- Body -->
    <!-- ===================== -->
    <article class="project-body">
      <!-- Overview -->
      <section>
        <h2>Overview</h2>

        <p>
        I worked with MIT’s Personal Robotics Group on a human–robot interaction (HRI) user study focused on storytelling interactions with the social robot Jibo. The project explored how interaction design and robot behavior influence how people perceive and engage with embodied robots.
        </p>
      </section>

      <!-- My Role-->
      <section class="section">
        <h2>My Role</h2>
        <p >
        I contributed to the execution and analysis of a large-scale human–robot interaction user study involving the social robot Jibo, with a focus on study logistics, participant interaction, and results synthesis.       
        </p>

        <ul class="detail-list">
            <li>Coordinated a 50+ participant user study, including preparing and shipping Jibo research stations to participants’ homes.</li>
            <li>Hosted onboarding sessions and conducted participant interviews throughout the study.</li>
            <li>Provided remote technical support and troubleshooting to ensure consistent robot behavior across sessions.</li>
            <li>Assisted with data organization and analysis, and contributed to writing the results section of the paper accepted to HRI.</li>
        </ul>
        <p>This role required balancing experimental protocol with the realities of deploying and supporting embodied robots in real-world, at-home settings.</p>
        </section>

     <!-- Study Execution -->
      <section class="section">
        <h2>Study Execution</h2>
        <p> Each participant received a preconfigured Jibo research station shipped to their home. 
        During the study, I helped host onboarding meetings which walked users through how to set-up their Jibo, interact with the app and Jibo itself, and would often live trouble-shoot any issues.</p>
        <p> Study sessions followed a structured interaction flow, where participants engaged in a guided storytelling experience mediated by the robot. The robot prompted participants to share personal stories and responded by presenting curated narratives attributed to other people, framing the robot as a social proxy rather than an autonomous storyteller. Participants interacted with the robot verbally while also using a tablet-based interface to provide ratings and feedback during the session.</p>
        <p>I helped conduct structured interviews throughout the study to capture qualitative feedback on user experience, interpretation of the robot’s storytelling behavior, and perceived social connection. Participants also completed surveys measuring changes in social connection and empathy. Together, these qualitative and quantitative measures informed the analysis and results presented in the final paper.</p>
        </section>

        <section class="section">
        <h2>Data Analysis & Results</h2>
        <p>Study data were analyzed using a mixed-methods approach combining quantitative survey analysis with qualitative interview coding. Pre- and post-study survey responses were used to evaluate changes in participants’ reported social connection and empathy across experimental conditions, with comparisons made between the social proxy condition (where stories were explicitly framed as originating from other people) and the social agent condition (where the robot presented stories as its own).</p>
        <p>In parallel, interview responses were coded and analyzed to identify recurring themes related to how participants interpreted the robot’s role, attributed social meaning to the stories, and reflected on their sense of connection to others. This qualitative analysis helped explain some of the quantitative trends, such as participants forming connections across shared human experiences and responding differently when the robot was positioned as a mediator rather than an autonomous social actor.</p>
        <p>I contributed to organizing and analyzing both survey and interview data, and helped synthesize these findings into the results section of the paper. Together, the quantitative and qualitative analyses demonstrated that framing robots as social proxies can meaningfully influence user perception and enhance social connection, grounding the reported results in both statistical evidence and participant experience.</p>
        </section>

        <section class="section">
        <h2>Related Applied User Studies</h2>
        <p>Alongside this work, I contributed to participatory workshops with the Affective Computing Group, where we deployed interactive web-based storytelling tools with foster youth in real-world, community-based settings. A picture from one of these workshops with the foster parents is shown above.</p>
        <p>I helped facilitate workshops, supported interface testing, and observed user interactions. I also helped bring in the Jibo platform developed in the Personal Robotics Lab and see if we could observe similar trends in foster youth.</p>
        <p></p>
        </section>
        

        </div>
        

        </section>
    </article>
  </section>
</Layout>

<script is:inline>
(() => {
  const root = document.querySelector("[data-gallery]");
  if (!root) return;

  const track = root.querySelector(".gallery-track");
  const slides = [...root.querySelectorAll(".gallery-slide")];
  const prev = root.querySelector("[data-prev]");
  const next = root.querySelector("[data-next]");
  const dotsWrap = root.querySelector("[data-dots]");

  if (!track || slides.length === 0) return;

  let idx = 0;

  function setActiveDot() {
    if (!dotsWrap) return;
    [...dotsWrap.children].forEach((d, j) => d.toggleAttribute("data-active", j === idx));
  }

  function snapTo(i) {
    idx = (i + slides.length) % slides.length;

    // Prevent vertical jump by forcing "block: nearest"
    slides[idx].scrollIntoView({
      behavior: "smooth",
      inline: "start",
      block: "nearest",
    });

    setActiveDot();
  }

  if (dotsWrap) {
    dotsWrap.innerHTML = "";
    slides.forEach((_, i) => {
      const b = document.createElement("button");
      b.type = "button";
      b.className = "gallery-dot";
      b.addEventListener("click", (e) => {
        e.preventDefault();
        snapTo(i);
      });
      dotsWrap.appendChild(b);
    });
  }

  prev?.addEventListener("click", (e) => {
    e.preventDefault();
    snapTo(idx - 1);
  });

  next?.addEventListener("click", (e) => {
    e.preventDefault();
    snapTo(idx + 1);
  });

  // When the gallery is focused, use arrow keys to navigate WITHOUT scrolling the page
  root.addEventListener("keydown", (e) => {
    if (e.key === "ArrowLeft") {
      e.preventDefault();
      snapTo(idx - 1);
    } else if (e.key === "ArrowRight") {
      e.preventDefault();
      snapTo(idx + 1);
    }
  });

  // Make sure the gallery can receive keyboard focus
  if (!root.hasAttribute("tabindex")) root.setAttribute("tabindex", "0");

  snapTo(0);

    const videos = document.querySelectorAll("video.lazy-video[data-src]");
    if (!videos.length) return;

    const loadVideo = (video) => {
      const src = video.dataset.src;
      if (!src) return;
      video.src = src;
      video.removeAttribute("data-src");
      // video.load(); // optional; setting src usually triggers load
    };

    if (!("IntersectionObserver" in window)) {
      videos.forEach(loadVideo);
      return;
    }

    const io = new IntersectionObserver((entries) => {
      entries.forEach((entry) => {
        if (!entry.isIntersecting) return;
        loadVideo(entry.target);
        io.unobserve(entry.target);
      });
    }, { rootMargin: "200px 0px" }); // starts loading slightly before it appears

    videos.forEach(v => io.observe(v));
})();
</script>
<style>

/* Clean bullet style */
.detail-list {
  margin-top: 18px;
  padding-left: 1.1rem;
  max-width: 80ch;
}

.detail-list li {
  margin: 12px 0;
  line-height: 1.6;
}

.detail-list strong {
  font-weight: 700;
}

</style>