---
import Layout from "../../../components/Layout.astro";
const BASE = import.meta.env.BASE_URL;

// Edit these strings later
const project = {
  title: "Autonomous Bin Retrieval Robot",
  subtitle: "Final Project for Introduction to Robotics (2.12)",
  date: "May 2025",
  category: "Embedded Systems, Perception, Localization, Controls",
   tools: [
    "Python",
    "C++ (PlatformIO)",
    "OpenCV",
    "Jetson Nano",
    "ESP32",
    "PySerial",
  ],
  links: [
    { label: "Github", href: "https://github.com/radkinz/2.12-MobileRobot" },
  ],

gallery: [
  { src: `${BASE}projects/mobile-robot/hero-1.png`, mode: "contain" },
  { src: `${BASE}projects/mobile-robot/hero-2.jpg`, mode: "contain" },
{ src: `${BASE}projects/mobile-robot/hero-3.png`, mode: "contain" },
  { src: `${BASE}projects/mobile-robot/hero-4.jpg`, mode: "cover" },

],

};
---

<Layout title={`${project.title} — River Adkins`}>
  <section class="panel project-page">
    <!-- ===================== -->
    <!-- Back -->
    <!-- ===================== -->
    <a class="back" href={`${import.meta.env.BASE_URL}#projects`}>← Back to projects</a>

    <!-- ===================== -->
    <!-- Header -->
    <!-- ===================== -->
    <header class="project-header">
      <h1 class="project-title">{project.title}</h1>
      <p class="project-subtitle">{project.subtitle}</p>
    </header>

    <!-- ===================== -->
    <!-- Top: Gallery + Info -->
    <!-- ===================== -->
    <div class="project-top">
      <!-- Gallery -->
      <div class="gallery" data-gallery>
        <div class="gallery-track">
          {project.gallery.map((item) => (
            <div class={`gallery-slide ${item.mode === "contain" ? "is-poster" : ""}`}>
                <img src={item.src} alt="" loading="lazy" />
            </div>
            ))}

        </div>

        <div class="gallery-controls">
        <button type="button" class="gallery-btn" data-prev aria-label="Previous">←</button>
        <button type="button" class="gallery-btn" data-next aria-label="Next">→</button>
        </div>


        <div class="gallery-dots" data-dots></div>
      </div>

      <!-- Project info card -->
      <aside class="project-card">
        <h2 class="project-card-title">Project Info</h2>
        <hr />
        <p><strong>Category:</strong> {project.category}</p>
        <p><strong>Project date:</strong> {project.date}</p>
        <p><strong>Tools used:</strong> {project.tools.join(", ")}</p>

        <div class="project-links">
                <a
            class="github-badge"
            href="https://github.com/radkinz/2.12-MobileRobot"
            target="_blank"
            rel="noopener noreferrer"
            >
            <svg class="github-icon" viewBox="0 0 24 24" aria-hidden="true">
                <path
                fill="currentColor"
                d="M12 .5C5.73.5.5 5.73.5 12c0 5.09 3.29 9.39 7.86 10.91.57.1.78-.25.78-.56v-2.1c-3.2.7-3.88-1.54-3.88-1.54-.53-1.34-1.3-1.7-1.3-1.7-1.06-.72.08-.7.08-.7 1.17.08 1.79 1.2 1.79 1.2 1.04 1.78 2.73 1.27 3.4.97.1-.75.4-1.27.73-1.56-2.55-.29-5.23-1.28-5.23-5.69 0-1.26.45-2.29 1.19-3.1-.12-.29-.52-1.46.11-3.05 0 0 .97-.31 3.18 1.18a11.1 11.1 0 0 1 5.8 0c2.2-1.49 3.17-1.18 3.17-1.18.63 1.59.23 2.76.11 3.05.74.81 1.19 1.84 1.19 3.1 0 4.42-2.68 5.4-5.24 5.69.41.35.78 1.04.78 2.1v3.11c0 .31.21.66.79.55A11.51 11.51 0 0 0 23.5 12C23.5 5.73 18.27.5 12 .5z"
                />
            </svg>
            <span>2.12-MobileRobot</span>
            </a>


        </div>
      </aside>
    </div>

    <!-- ===================== -->
    <!-- Body -->
    <!-- ===================== -->
    <article class="project-body">
      <!-- Project Description -->
      <section>
        <h2>Project Description</h2>

         <p>
    Built for the final competition in MIT’s 2.12 (Introduction to Robotics) class, this robot autonomously located,
    retrieved, and began placing color-coded bins in a structured arena using a full-stack pipeline: AprilTag-based
    localization, sensor fusion, short-range ToF sensing, and a high-torque bin-dragging end effector.
  </p>

  <p>
    The platform was a two-wheeled rear-driven differential-drive robot (tank-style) with a custom scoop-style
    end effector powered by a 60 RPM high-torque servo to reliably capture and drag bins. A calibrated front camera
    on a Jetson Nano handled AprilTag pose estimation and vision processing, while the robot relied on wheel encoders,
    an IMU, and ToF sensors for robust motion and proximity behavior.
  </p>

  <div class="callout">
    <strong>Key capabilities:</strong>
    <ul>
      <li>AprilTag-based localization + mapping in a structured arena</li>
      <li>Sensor fusion (AprilTag + IMU + odometry) for stable pose estimates</li>
      <li>Vision-based bin classification with ToF-assisted bin approach</li>
      <li>Wireless Jetson to ESP32 serial communiction and manual override for debugging/teleop</li>
      <li>High-torque servo end effector designed for consistent bin capture</li>
    </ul>
  </div>
      </section>

      <!-- Software Architecture -->
      <section>
  <h2>Software Architecture</h2>

  <p>
    The stack was split cleanly into high-level autonomy on the Jetson and real-time embedded control on the ESP32.
    The Jetson ran perception, localization, and task logic in Python (OpenCV + NumPy/SciPy), then issued motion and
    actuator commands to the ESP32. The ESP32 firmware (C++ via PlatformIO) handled closed-loop motor control,
    synchronous sensor polling, and wireless telemetry with a modular, test-first codebase.
  </p>

  <div class="callout">
    <strong>High-level (Jetson / Python):</strong>
    <ul>
      <li><code>apriltag_pose.py</code> — calibrated AprilTag 6DOF pose estimation</li>
      <li><code>updatedSensorFusion.py</code> — fused AprilTag + IMU for localization robustness</li>
      <li><code>sensorSuite.py</code> — unified access to IMU, ToF, and serial comms</li>
      <li><code>binIdentify.py</code> / <code>binGrabTest.py</code> — bin classification + ToF-driven approach/grab routines</li>
      <li><code>targetTest.py</code> — primary run loop coordinating localization + bin acquisition behavior</li>
    </ul>
  </div>

  <div class="callout">
    <strong>Low-level (ESP32 / C++ PlatformIO):</strong>
    <ul>
      <li>2-wheel tank drive with PID velocity control and differential-drive kinematics</li>
      <li>Modular libraries: <code>PID</code>, <code>EncoderVelocity</code>, <code>MotorDriver</code>, <code>IMU</code>, <code>ToFSensor</code></li>
      <li>Wireless link via ESP-NOW using shared packet structs (<code>wireless.h</code>)</li>
      <li>Robot/manual controller split: controller ESP32 sends input to the robot's ESP32 executes motion overriding the autonomous state machine.</li>
      <li>Unit-style test programs for motors, encoders, sensors, and wireless reliability</li>
    </ul>
  </div>

  <p>
    Due to late-stage time constraints, the competition workflow was semi-autonomous. We relied on the core
    localization and bin grab/place loops, and used manual override for small corrections when needed.
  </p>

  <p>
   I led the robot’s autonomy stack from perception to actuation. This included designing the ESP32 firmware (motor drivers, encoder feedback, PID velocity control, and wireless protocols) and integrating and extending the Jetson-side Python code to build the semi-autonomous localization and bin-retrieval loops.
  </p>
</section>


      <!-- Performance & Design -->
      <section class="project-section">
        <h2>Autonomous Task Execution</h2>
        <div>
        <div class="video-wrap">
         <video
        class="lazy-video"
        controls
        playsinline
        preload="none"
        muted
        data-src={`${import.meta.env.BASE_URL}projects/mobile-robot/binGrab.MP4`} 
        type="video/mp4" 
        poster={`${import.meta.env.BASE_URL}projects/mobile-robot/video1-poster.png`} 
        >
        Sorry, your browser doesn’t support embedded videos.
        </video>

        </div>

        <p class="caption">
        Robot autonomously grabbing a bin using AprilTag localization.        
        </p>
        </div>

                <div class="video-wrap">
      
        <video
        class="lazy-video"
        controls
        playsinline
        preload="none"
        muted
        data-src={`${import.meta.env.BASE_URL}projects/mobile-robot/binPlace.MP4`} 
        type="video/mp4" 
        poster={`${import.meta.env.BASE_URL}projects/mobile-robot/video2-poster.png`} 
        >
        Sorry, your browser doesn’t support embedded videos.
        </video>


        <p class="caption">
        Autonomous bin placement in the designated scoring zone.     
        </p>
        </div>
        

        </section>
    </article>
  </section>
</Layout>

<script is:inline>
(() => {
  const root = document.querySelector("[data-gallery]");
  if (!root) return;

  const track = root.querySelector(".gallery-track");
  const slides = [...root.querySelectorAll(".gallery-slide")];
  const prev = root.querySelector("[data-prev]");
  const next = root.querySelector("[data-next]");
  const dotsWrap = root.querySelector("[data-dots]");

  if (!track || slides.length === 0) return;

  let idx = 0;

  function setActiveDot() {
    if (!dotsWrap) return;
    [...dotsWrap.children].forEach((d, j) => d.toggleAttribute("data-active", j === idx));
  }

  function snapTo(i) {
    idx = (i + slides.length) % slides.length;

    // Prevent vertical jump by forcing "block: nearest"
    slides[idx].scrollIntoView({
      behavior: "smooth",
      inline: "start",
      block: "nearest",
    });

    setActiveDot();
  }

  if (dotsWrap) {
    dotsWrap.innerHTML = "";
    slides.forEach((_, i) => {
      const b = document.createElement("button");
      b.type = "button";
      b.className = "gallery-dot";
      b.addEventListener("click", (e) => {
        e.preventDefault();
        snapTo(i);
      });
      dotsWrap.appendChild(b);
    });
  }

  prev?.addEventListener("click", (e) => {
    e.preventDefault();
    snapTo(idx - 1);
  });

  next?.addEventListener("click", (e) => {
    e.preventDefault();
    snapTo(idx + 1);
  });

  // When the gallery is focused, use arrow keys to navigate WITHOUT scrolling the page
  root.addEventListener("keydown", (e) => {
    if (e.key === "ArrowLeft") {
      e.preventDefault();
      snapTo(idx - 1);
    } else if (e.key === "ArrowRight") {
      e.preventDefault();
      snapTo(idx + 1);
    }
  });

  // Make sure the gallery can receive keyboard focus
  if (!root.hasAttribute("tabindex")) root.setAttribute("tabindex", "0");

  snapTo(0);

    const videos = document.querySelectorAll("video.lazy-video[data-src]");
    if (!videos.length) return;

    const loadVideo = (video) => {
      const src = video.dataset.src;
      if (!src) return;
      video.src = src;
      video.removeAttribute("data-src");
      // video.load(); // optional; setting src usually triggers load
    };

    if (!("IntersectionObserver" in window)) {
      videos.forEach(loadVideo);
      return;
    }

    const io = new IntersectionObserver((entries) => {
      entries.forEach((entry) => {
        if (!entry.isIntersecting) return;
        loadVideo(entry.target);
        io.unobserve(entry.target);
      });
    }, { rootMargin: "200px 0px" }); // starts loading slightly before it appears

    videos.forEach(v => io.observe(v));
})();
</script>