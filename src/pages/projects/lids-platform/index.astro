---
import Layout from "../../../components/Layout.astro";
const BASE = import.meta.env.BASE_URL;

// Edit these strings later
const project = {
  title: "Mobile Autonomy Platform",
  subtitle: "Research project conducted with the Laboratory for Information and Decision Systems (LIDS)",
  date: "Spring 2025",
  category: "System Design, Perception, Controls",
  tools: ["ROS 2", "Python", "Flask", "OpenCV","Jupyter Notebook", "OnShape"],
  links: [

  ],
gallery: [
  { src: `${BASE}projects/lids-platform/hero-1.JPG`, mode: "cover" },
  { src: `${BASE}projects/lids-platform/hero-2.png`, mode: "contain" },
  { src: `${BASE}projects/lids-platform/hero-3.png`, mode: "cover" },
],

};
---

<Layout title={`${project.title} — River Adkins`}>
  <section class="panel project-page">
    <!-- ===================== -->
    <!-- Back -->
    <!-- ===================== -->
    <a class="back" href={`${import.meta.env.BASE_URL}#projects`}>← Back to projects</a>

    <!-- ===================== -->
    <!-- Header -->
    <!-- ===================== -->
    <header class="project-header">
      <h1 class="project-title">{project.title}</h1>
      <p class="project-subtitle">{project.subtitle}</p>
    </header>

    <!-- ===================== -->
    <!-- Top: Gallery + Info -->
    <!-- ===================== -->
    <div class="project-top">
      <!-- Gallery -->
      <div class="gallery" data-gallery>
        <div class="gallery-track">
          {project.gallery.map((item) => (
            <div class={`gallery-slide ${item.mode === "contain" ? "is-poster" : ""}`}>
                <img src={item.src} alt="" loading="lazy" />
            </div>
            ))}

        </div>

        <div class="gallery-controls">
        <button type="button" class="gallery-btn" data-prev aria-label="Previous">←</button>
        <button type="button" class="gallery-btn" data-next aria-label="Next">→</button>
        </div>


        <div class="gallery-dots" data-dots></div>
      </div>

      <!-- Project info card -->
      <aside class="project-card">
        <h2 class="project-card-title">Project Info</h2>
        <hr />
        <p><strong>Category:</strong> {project.category}</p>
        <p><strong>Project date:</strong> {project.date}</p>
        <p><strong>Tools used:</strong> {project.tools.join(", ")}</p>

        <div class="project-links">
          {project.links.map((l) => (
            <a
              class="link"
              href={l.href}
              target={l.href.startsWith("http") ? "_blank" : undefined}
              rel="noreferrer"
            >
              {l.label} →
            </a>
          ))}
        </div>
      </aside>
    </div>

    <!-- ===================== -->
    <!-- Body -->
    <!-- ===================== -->
    <article class="project-body">
      <section>
        <h2>Project Overview</h2>

        <p>
          During Spring 2025, I built a mobile robot as part of my work with the MIT Laboratory for Information and Decision Systems (LIDS). The goal was to design and implement a system that could reliably support autonomy and real-time control for the purpose of supporting future research into human–robot collaboration.
        </p>

        <p>
          This work culminated in a fully integrated robot capable of teleoperation through a web interface and autonomous behaviors such as AprilTag following.
        </p>

      </section>

      <!-- Hardware Platform -->
      <section>
        <h2>Hardware Platform</h2>
        <p>The robot platform integrates onboard compute, sensing, and actuation into a mobile system designed to support autonomy research and testing.
            The chassis and wheels are from the Yahboom X3 mobile robot kit, which I upgraded to support
            additional hardware.
        </p>
        <p> Key Components: </p>
        <ul class="arch-list">
          <li><strong>Onboard compute:</strong> NVIDIA Jetson AGX Orin for perception, control, and autonomy</li>
          <li><strong>Mobile base:</strong> Mecanum-drive platform enabling omnidirectional motion</li>
          <li><strong>Sensing:</strong> Forward-facing ZED 2 stereo camera for visual perception </li>
          <li><strong>Low-level interface:</strong> Yahboom Expansion Board for motor control and hardware I/O</li>
        </ul>
      </section>

      <!-- =========================
     Software Architecture
========================= -->
<section id="software-architecture">
  <h2>Software Architecture</h2>

  <p>
    The robot software stack was built in ROS 2 and organized as a set of modular nodes for
    perception, state estimation, control, and hardware execution. The goal was to support rapid iteration and flexibility for any experiment needed.
  </p>

  <div class="arch-grid">
    <!-- Left: text -->
    <div>
      <h3>Node-based system design</h3>
      <p>
        At a high level, sensor data flows through perception and estimation nodes to produce state information,
        which is consumed by behavior controllers that publish velocity commands for the low-level motor interface.
      </p>

      <ul class="arch-list">
        <li>
          <strong>Perception:</strong> AprilTag detection node to publish tag pose estimates for downstream behaviors.
        </li>
        <li>
          <strong>State estimation:</strong> IMU filtering (<code>imu_filter_madgwick</code>) and odometry fused via
          an Extended Kalman Filter to provide a stable pose estimate.
        </li>
        <li>
          <strong>Hardware interface:</strong> Bringup and motor driver node translating <code>cmd_vel</code> into low-level motor commands that interfaced with the Yahboom expansion board.
        </li>
      </ul>

        <div class="diagram-card">
          <img
        src={`${import.meta.env.BASE_URL}projects/lids-platform/node_graph_topic_active.png`}
        alt="ROS2 node graph showing IMU filtering, odometry, robot_localization, cmd_vel, and driver interface."
        loading="lazy"
      />
          <p class="diagram-caption">
        ROS 2 node graph (topics active): perception + estimation feeding behavior nodes that publish <code>cmd_vel</code> to the driver.
          </p>
        </div>

      <h3>April Tag Following</h3>
      <p>
      AprilTag following was implemented as a ROS 2 behavior package that closes the loop between perception and motion. Nodes within this package subscribe to tag pose estimates, compute distance and alignment error, and publish smooth velocity commands to center and approach the target. When no tag is detected, the system transitions to a fallback search behavior where the robot rotates until it reestablishes visual contact.
      </p>
      <h3>Testing utilities</h3>
      <p>
        In parallel, a separate ROS 2 motion utilities package provided configurable motion primitives such as spinning, strafing, and following a relative vector. These tools were used to validate actuation, tune velocity parameters, and debug the end-to-end command pipeline.
        This package also set the foundation for the minimal web application that I built to support human interaction.
      </p>
        <div class="video-wrap">
      <video
        class="lazy-video"
        controls
        playsinline
        preload="none"
        muted
        data-src={`${import.meta.env.BASE_URL}projects/lids-platform/Square.mp4`} 
        type="video/mp4" 
        poster={`${import.meta.env.BASE_URL}projects/lids-platform/square-poster.png`} 
        >
        Sorry, your browser doesn’t support embedded videos.
        </video>

        <p class="caption">
            The robot executing a motion primitive of driving in a square with a given length in meters.
        </p>
        </div>
    </div>

  </div>
</section>

<section class="two-col">
  
  <div>
  <h2>Web-Based Control</h2>
  <p>
    To support rapid testing, demonstrations, and human interaction, I developed
    a lightweight web-based control interface using Flask. The interface provides a simple way
    to issue commands to the robot and observe system behavior without requiring direct access
    to the ROS 2 environment.
  </p>

  <p>
    TThe Flask application serves as a bridge between browser-based user input and ROS 2 topics such as <code>cmd_vel</code>. This made it
    possible to control the robot from any browser which greatly helped debug and test the robot's actions.
  </p>
  </div>

  <div class="diagram-card">
          <img
        src="/projects/lids-platform/UI.jpg"
        alt="A picture of the Flask app's UI."
        loading="lazy"
      />
          <p class="diagram-caption">
        The user interface for the web application. 
          </p>
        </div>

</section>


      <section class="project-section">
        <h2>System Demonstration</h2>

         <div class="video-wrap">
      <video
        class="lazy-video"
        controls
        playsinline
        preload="none"
        muted
        data-src={`${import.meta.env.BASE_URL}projects/lids-platform/AprilTagDemo.mp4`} 
        type="video/mp4" 
        poster={`${import.meta.env.BASE_URL}projects/lids-platform/AprilTagDemo-poster.png`} 
        >
        Sorry, your browser doesn’t support embedded videos.
        </video>

        <p class="caption">
            The robot following an April Tag taped to my leg.
        </p>
        </div>

         <div class="video-wrap">
      <video
        class="lazy-video"
        controls
        playsinline
        preload="none"
        muted
        data-src={`${import.meta.env.BASE_URL}projects/lids-platform/WebDemo.mp4`} 
        type="video/mp4" 
        poster={`${import.meta.env.BASE_URL}projects/lids-platform/WebDemo-poster.png`} 
        >
        Sorry, your browser doesn’t support embedded videos.
        </video>

        <p class="caption">
        Given the vector inputted on the Flask UI, the robot swiftly executes the command.
        </p>
        </div>

        </section>


      <!-- Future Work -->
      <section class="proj-section">
        <h2 class="proj-h2">Future Work</h2>
        <p>
        This platform directly enabled my subsequent research during the summer and fall. After establishing reliable motion primitives, perception pipelines, and a web-based control interface, the project shifted toward studying how to enhance human-robot collaboration.
        </p>
        <p>
        Ongoing work builds on this system by integrating speech prosody and large language models into the autonomy pipeline. The goal is to move beyond direct command execution toward interaction-aware control, where the robot adapts its behavior based on how instructions are delivered. This work will culminate in a user study and research paper evaluating prosody-informed robot decision-making.
        </p>
      </section>
    </article>
  </section>
</Layout>

<script is:inline>
(() => {
  const root = document.querySelector("[data-gallery]");
  if (!root) return;

  const track = root.querySelector(".gallery-track");
  const slides = [...root.querySelectorAll(".gallery-slide")];
  const prev = root.querySelector("[data-prev]");
  const next = root.querySelector("[data-next]");
  const dotsWrap = root.querySelector("[data-dots]");

  if (!track || slides.length === 0) return;

  let idx = 0;

  function setActiveDot() {
    if (!dotsWrap) return;
    [...dotsWrap.children].forEach((d, j) => d.toggleAttribute("data-active", j === idx));
  }

  function snapTo(i) {
    idx = (i + slides.length) % slides.length;

    // Prevent vertical jump by forcing "block: nearest"
    slides[idx].scrollIntoView({
      behavior: "smooth",
      inline: "start",
      block: "nearest",
    });

    setActiveDot();
  }

  if (dotsWrap) {
    dotsWrap.innerHTML = "";
    slides.forEach((_, i) => {
      const b = document.createElement("button");
      b.type = "button";
      b.className = "gallery-dot";
      b.addEventListener("click", (e) => {
        e.preventDefault();
        snapTo(i);
      });
      dotsWrap.appendChild(b);
    });
  }

  prev?.addEventListener("click", (e) => {
    e.preventDefault();
    snapTo(idx - 1);
  });

  next?.addEventListener("click", (e) => {
    e.preventDefault();
    snapTo(idx + 1);
  });

  // When the gallery is focused, use arrow keys to navigate WITHOUT scrolling the page
  root.addEventListener("keydown", (e) => {
    if (e.key === "ArrowLeft") {
      e.preventDefault();
      snapTo(idx - 1);
    } else if (e.key === "ArrowRight") {
      e.preventDefault();
      snapTo(idx + 1);
    }
  });

  // Make sure the gallery can receive keyboard focus
  if (!root.hasAttribute("tabindex")) root.setAttribute("tabindex", "0");

  snapTo(0);

    const videos = document.querySelectorAll("video.lazy-video[data-src]");
    if (!videos.length) return;

    const loadVideo = (video) => {
      const src = video.dataset.src;
      if (!src) return;
      video.src = src;
      video.removeAttribute("data-src");
      // video.load(); // optional; setting src usually triggers load
    };

    if (!("IntersectionObserver" in window)) {
      videos.forEach(loadVideo);
      return;
    }

    const io = new IntersectionObserver((entries) => {
      entries.forEach((entry) => {
        if (!entry.isIntersecting) return;
        loadVideo(entry.target);
        io.unobserve(entry.target);
      });
    }, { rootMargin: "200px 0px" }); // starts loading slightly before it appears

    videos.forEach(v => io.observe(v));
})();
</script>